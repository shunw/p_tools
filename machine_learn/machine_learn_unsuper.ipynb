{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means\n",
    "\n",
    "- not behave very well when the blobs have very different diameters\n",
    "\n",
    "- hard clustering => assign each instance to a single cluster\n",
    "\n",
    "- soft clustering => distance between the instance and the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters = k)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "kmeans.cluster_centers_ # get the cluster centers\n",
    "\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "kmeans.predict(X_new) # hard clustering --- just give the class\n",
    "kmeans.transform(X_new) # soft clustering --- give the distance of that point to each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigation of Converge not to the right solution\n",
    "\n",
    "- Centroid initialization methods (know approximately where the centroids should be)\n",
    "\n",
    "    ```\n",
    "    good_init = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n",
    "    kmeans = KMeans(n_clusters = 5, inti = good_init, n_init = 1)\n",
    "    ```\n",
    "\n",
    "- run the algorithm multiple times with different random initializations and keep the best one\n",
    "\n",
    "    - measure method: `kmeans.inertia_`\n",
    "\n",
    "    - measure socre: `kmeans.score(X)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch K-Means\n",
    "\n",
    "`from sklearn.cluster import MiniBatchKMeans`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimal # of clusters\n",
    "\n",
    "- plot the inertia vs clusters k => find the elbow\n",
    "\n",
    "- use silhouette_score function \n",
    "\n",
    "    ```\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    silhouette_score(X, kmeans.labels_)\n",
    "    ```\n",
    "\n",
    "- check with silhouette diagram, check both score and the size of the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "- Image/ Color Segmentation\n",
    "\n",
    "    ```\n",
    "    X = iamge.reshape(-1, 3)\n",
    "    kmeans = KMeans(n_cluster = 8).fit(X)\n",
    "    segmented_img = kmeans.cluster_ceters_[kmeans.labels_]\n",
    "    segmented_img = segmented_img.reshape(image.shape)\n",
    "    ```\n",
    "\n",
    "- Preprocessing\n",
    "\n",
    "    ```\n",
    "    pipeline = Pipeline([\n",
    "        ('kmeans', KMeans()), \n",
    "        ('log_reg', LogisticRegression())\n",
    "    ])\n",
    "\n",
    "    param_grid = dict(kmeans__n_clusters = range(2, 100))\n",
    "    grid_clf = GridSearchCV(pipeline, param_grid, cv = 3, verbose = 2)\n",
    "    grid_clf.fit(X_train, y_train)\n",
    "    ```\n",
    "\n",
    "- semi-supervised learning (plenty of unlabeled while few labeled)\n",
    "\n",
    "    - cluster the training sets into labeled instances\n",
    "\n",
    "    - label propagation: propagate the labels to all the other instances in the same cluster\n",
    "\n",
    "    - propate the 20% closest to the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the semi-supervised learning\n",
    "\n",
    "# cluster the tranining sets\n",
    "k = 50\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "X_digits_dist = kmeans.fit_transform(X_train)\n",
    "representative_digit_idx = np.argmin(X_digits_dist, axis=0)\n",
    "X_representative_digits = X_train[representative_digit_idx]\n",
    "\n",
    "# label manually\n",
    "y_representative_digits = np.array([4, 8, 0, 6, 8, 3, ..., 7, 6, 2, 3, 1, 1])\n",
    "\n",
    "# label propagation\n",
    "y_train_propagated = np.empty(len(X_train), dtype=np.int32)\n",
    "for i in range(k):\n",
    "    y_train_propagated[kmeans.labels_==i] = y_representative_digits[i]\n",
    "\n",
    "# 20% percentile_clostest\n",
    "percentile_closest = 20\n",
    "â€‹\n",
    "X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]\n",
    "for i in range(k):\n",
    "    in_cluster = (kmeans.labels_ == i)\n",
    "    cluster_dist = X_cluster_dist[in_cluster]\n",
    "    cutoff_distance = np.percentile(cluster_dist, percentile_closest)\n",
    "    above_cutoff = (X_cluster_dist > cutoff_distance)\n",
    "    X_cluster_dist[in_cluster & above_cutoff] = -1\n",
    "\n",
    "partially_propagated = (X_cluster_dist != -1)\n",
    "X_train_partially_propagated = X_train[partially_propagated]\n",
    "y_train_partially_propagated = y_train_propagated[partially_propagated]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|cluster-alg | pros | cons|\n",
    "|---|---|---|\n",
    "|k means| fast/ scalable | suboptimal <- run several times| \n",
    "| - | - | need to specify # of clusters| \n",
    "| - | - | impact if clusters have varying sizes| "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "It cannot do prediction but can predict with knn\n",
    "\n",
    "Need to set the esp properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.05)\n",
    "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# label all the instances\n",
    "dbscan.labels_\n",
    "dbscan.core_sample_indices_\n",
    "dbscan.components_\n",
    "\n",
    "# fit knn with the dbscan\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=50)\n",
    "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
    "\n",
    "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
    "knn.predict(X_new)\n",
    "\n",
    "# within the new data, it can make good predict, though the new data could be anomaly. \n",
    "knn.predict_proba(X_new)\n",
    "\n",
    "y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n",
    "\n",
    "y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
    "\n",
    "y_pred[y_dist > 0.2] = -1\n",
    "\n",
    "y_pred.ravel()"
   ]
  }
 ]
}