{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron -> Multiple Perceptrons (MLP)\n",
    "\n",
    "- perceptron is one of the simplest ANN (artificial neural network); MLP is the resulting ANN. \n",
    "\n",
    "- similar to Stochastic Gradient Descent\n",
    "\n",
    "- don't output a class probability, rather, they make a predictions based on a hard threshold. \n",
    "\n",
    "`from sklearn.linear_model import Perceptron`\n",
    "\n",
    "- MLP (with two hidden layers)\n",
    "\n",
    "    - scale first\n",
    "\n",
    "    - Sequential API\n",
    "        ```\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Flatten(input_shape = [28, 28]),\n",
    "            keras.layers.Dense(300, activation = 'relu'), \n",
    "            keras.layers.Dense(100, activation = 'relu'), \n",
    "            keras.layers.Dense(10, activation = 'softmax')\n",
    "        ])\n",
    "\n",
    "        model = keras.models.Sequential([\n",
    "            keras.layers.Dense(30, activation = 'relu', input_shape = X_train.shape[1:]), \n",
    "            keras.layers.Dense(1)\n",
    "        ])\n",
    "        ```\n",
    "    - compile the model\n",
    "\n",
    "        - loss: for sparse labels (0-9 exclusive) => sparse_categorcial_crossentropy; for one-hot vector => categorial_crossentropy; binary output => binary_crossentropy\n",
    "\n",
    "        - optimizer: sgd => Stochastic Gradient Descent <= need to tune the learning rate; \n",
    "\n",
    "            ```\n",
    "            model.compile(loss = 'sparse_categorical_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n",
    "\n",
    "            model.compile(loss = 'mean_squared_error', optimizer = 'sgd')\n",
    "            ```\n",
    "\n",
    "    - train data\n",
    "\n",
    "        ```\n",
    "        history = model.fit(X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid))\n",
    "        history.params\n",
    "        history.epochs\n",
    "        history.history \n",
    "\n",
    "        pd.DataFrame(history.history).plot(figsize = (8, 5))\n",
    "        plt.grid(True)\n",
    "        plt.gca().set_ylim(0, 1)\n",
    "        plt.show()\n",
    "        ```\n",
    "    - tune\n",
    "\n",
    "        - learning rate\n",
    "\n",
    "        - other optimizer (also need to tune learning rate)\n",
    "\n",
    "        - mode hyperparameter (# layers; # neurons; activation function; batch size)\n",
    "\n",
    "    - test\n",
    "\n",
    "        `model.evaluate(X_test, y_test)`\n",
    "\n",
    "    - some layer info\n",
    "        ```\n",
    "        h = model.layers[1]\n",
    "        h.name\n",
    "        model.get_layer('dense') is h => True\n",
    "        weights, biases = h.get_weights()\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsequential Neural Network (Functional API)\n",
    "\n",
    "- need both regression task and classification task\n",
    "\n",
    "- multiple independent tasks on the same data\n",
    "\n",
    "- add some auxiliary outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name = 'aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "model.compile(loss = ['mse', 'mse'], loss_weight = [.9, .1], optimizer = 'sgd) <- for the aux_output\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A, X_test_B], [y_test, y_test]) <- for aux_output\n",
    "\n",
    "y_pred = model.predict((X_new_A, X_new_B))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subclassing AIP => Dynamic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs) # handles standard args (e.g., name)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output\n",
    "\n",
    "model = WideAndDeepModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "[playground](https://playground.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Tensorflow\n",
    "\n",
    "[save&load](https://www.tensorflow.org/tutorials/keras/save_and_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Restore\n",
    "\n",
    "```\n",
    "model = keras.layers.Sequential([...])\n",
    "model.compile([...])\n",
    "model.fit([...])\n",
    "model.save('my_keras_model.h5')\n",
    "\n",
    "model = keras.models.load_model('model_name.h5')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "- early stopping \n",
    "    ```\n",
    "    checkpoint_cb = keras.ModelCheckpoint('my_keras_model.h5') \n",
    "    history = model.fit(X_train, y_train, epochs = 10, callbacks = [checkpoint_cb])\n",
    "    ```\n",
    "\n",
    "    - save_best_only = True -> only save the best one by checking the validation data\n",
    "\n",
    "    - use EarlyStopping\n",
    "\n",
    "        ```\n",
    "        early_stopping_cb = keras.callbacks.EarlyStopping(patience = 10, restore_best_weights = True)\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs = 100, validation_data = (X_valid, y_valid), callbacks = [checkpoint_cb, early_stopping_cb])\n",
    "        ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir() # e.g., './my_logs/run_2019_06_07-15_15_22\n",
    "\n",
    "# after build and compile the model\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs = 30, validation_data = (X_valid, y_valid), callbacks = [tensorboard_cb])"
   ]
  }
 ]
}