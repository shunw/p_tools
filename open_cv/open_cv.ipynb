{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notice:\n",
    "\n",
    "- waitKey(0)\n",
    "\n",
    "    - work in .py\n",
    "\n",
    "    - need to add the cv2.startWindowThread() after openning the startWindowThread\n",
    "    \n",
    "    - need to add cv2.destroyAllWindes();  cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1/17: Face Detection w/ web stream| photo| video\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opencv 3.3, Ryabnikov includes accurate, deep learning based face detector [face_detector](https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector)\n",
    "\n",
    "need two file: \n",
    "\n",
    "- .prototxt: defines the model architecture\n",
    "\n",
    "- .caffemodel: contains the weights for the actual layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection for Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-1"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "args = {'prototxt': './simple-object-tracking/deploy.prototxt', \n",
    "    'model': './simple-object-tracking/res10_300x300_ssd_iter_140000.caffemodel', \n",
    "    'image':'./test_1.jpeg', \n",
    "    'confidence': .5}\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(args['prototxt'], args['model'])\n",
    "\n",
    "# load the input image and construct an input blob for the image by resizing to a fixed 300 * 300 pixels and then normalizing it\n",
    "image = cv2.imread(args['image'])\n",
    "cv2.startWindowThread()\n",
    "\n",
    "(h, w) = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "# pass the blob through the network and obtain the detections and predictions\n",
    "net.setInput(blob)\n",
    "detections = net.forward()\n",
    "\n",
    "# loop over the detections \n",
    "for i in range(0, detections.shape[2]): \n",
    "\n",
    "    # extract the confidence associated with the prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    # filter out weak detections by ensuring the 'confidence' is greater than the minimum confidence\n",
    "    if confidence > args['confidence']: \n",
    "\n",
    "        # compute the (x, y) - coordinates of the bounding box for the object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype('int')\n",
    "\n",
    "        # draw the bounding box of the face along with the associated probability\n",
    "        text = '{:.2f}%'.format(confidence * 100)    \n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "        cv2.putText(image, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, .45, (0, 0, 255), 2)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow('Output', image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face detection for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "args = {'prototxt': './simple-object-tracking/deploy.prototxt', \n",
    "    'model': './simple-object-tracking/res10_300x300_ssd_iter_140000.caffemodel', \n",
    "    'video':'./test.mp4', \n",
    "    'confidence': .5}\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(args['prototxt'], args['model'])\n",
    "\n",
    "# open a pointer to the video stream and start the FPS timer\n",
    "stream = cv2.VideoCapture(args['video'])\n",
    "fps = FPS().start()\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "\n",
    "    # grab the frame from the threaded video file stream\n",
    "    (grabbed, frame) = stream.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed: \n",
    "        break\n",
    "\n",
    "    # resize the frame and covert it to grayscale (while still retaining 3 channels)\n",
    "    frame = imutils.resize(frame, width = 400)\n",
    "\n",
    "    # grab the frame dimensions and covert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "    # pass the blob throught the network and obtain the detections and preditions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    #loop over the detections\n",
    "    for i in range(0, detections.shape[2]): \n",
    "\n",
    "        # extract the confidence associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the 'confidence' is greater than the minimum confidence\n",
    "        if confidence < args['confidence']: \n",
    "            continue\n",
    "        \n",
    "        # compute the (x, y)- coordinates of the bounding obx for the object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype('int')\n",
    "\n",
    "        # draw the bounding box of the face along with the associated probability\n",
    "        text = '{:.2f}%'.format(confidence * 100)    \n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, .45, (0, 0, 255), 2)\n",
    "\n",
    "    # show the frame and update the FPS counter\n",
    "    cv2.imshow('Frame', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    fps.update()\n",
    "\n",
    "    # if the 'q' key was pressed, break from the loop\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "stream.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2/17 Deal with Images\n",
    "\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2018/07/19/opencv-tutorial-a-guide-to-learn-opencv/)\n",
    "\n",
    "- deal cv2 and imutils with basic image skills\n",
    "\n",
    "- deal cv2 with the erode and mask etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holistically-Nested Edge Detection\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2019/03/04/holistically-nested-edge-detection-with-opencv-and-deep-learning/)\n",
    "\n",
    "The main method: \n",
    "\n",
    "- blob: train the image with `dnn.blobFromImage` function. \n",
    "\n",
    "- net: dnn and read from caffe, and register the CropLayer (self defined) into the dv2.dnn_registerLayer\n",
    "\n",
    "    - the deploy.prototxt and hed_pretrained_bsds.caffemodel\n",
    "\n",
    "- use the blob into net and get the result\n",
    "\n",
    "### with Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(object): \n",
    "    def __init__(self, params, blobs): \n",
    "        # initialize our starting and ending (x, y) - coordinates of the crop\n",
    "        self.startX = 0\n",
    "        self.startY = 0\n",
    "        self.endX = 0\n",
    "        self.endY = 0\n",
    "    \n",
    "    def getMemoryShapes(self, inputs): \n",
    "        # the crop layer will receive two inputs -- we need to crop the first input blob to match the shape of the second one, keeping the batch size and number of channels\n",
    "        (inputShape, targetShape) = (inputs[0], inputs[1])\n",
    "        (batchSize, numChannels) = (inputShape[0], inputShape[1])\n",
    "        (H, W) = (targetShape[2], targetShape[3])\n",
    "\n",
    "        # compute the starting and ending crop coordinates\n",
    "        self.startX = int((inputShape[3] - targetShape[3]) / 2)\n",
    "        self.startY = int((inputShape[2] - targetShape[2]) / 2)\n",
    "        self.endX = self.startX + W\n",
    "        self.endY = self.startY + H\n",
    "\n",
    "        # return the shape of the volume (we will preform the actual crop during the forward pass)\n",
    "        return [[batchSize, numChannels, H, W]]\n",
    "    \n",
    "    def forward(self, inputs): \n",
    "        # use the derived (x, y)-coordinates to perform the crop\n",
    "        return [inputs[0][:, :, self.startY:self.endY, self.startX: self.endX]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "test_img = './holistically-nested-edge-detection/images/cat.jpg'\n",
    "args = {'image': 'test_1.jpeg', 'edge_detector': './holistically-nested-edge-detection/hed_model'}\n",
    "\n",
    "roots = './holistically-nested-edge-detection/hed_model'\n",
    "protoPath = os.path.join(args['edge_detector'], 'deploy.prototxt')\n",
    "modelPath = os.path.join(args['edge_detector'], 'hed_pretrained_bsds.caffemodel')\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# register our new layer with the model\n",
    "cv2.dnn_registerLayer('Crop', CropLayer)\n",
    "\n",
    "# load the input image and grab its dimensions\n",
    "image = cv2.imread(args['image'])\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# convert the image to grayscale, blur it, and perform Canny edge detection\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "canny = cv2.Canny(blurred, 30, 150)\n",
    "\n",
    "# construct a blob out of the input image for the Holistically-Nested Edge Detector\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor = 1.0, size = (W, H), mean = (104.00698793, 116.66876762, 122.67891434), swapRB = False, crop = False)\n",
    "\n",
    "# set the blob as the input to the network and perform a forward pass to compute the edges\n",
    "net.setInput(blob)\n",
    "hed = net.forward()\n",
    "hed = cv2.resize(hed[0, 0], (W, H))\n",
    "hed = (255 * hed).astype('uint8')\n",
    "\n",
    "# show the output edge detection results for Canny and Holistically-Nested Edge Detection\n",
    "cv2.imshow('Input', image)\n",
    "cv2.imshow('Canny', canny)\n",
    "cv2.imshow('HED', hed)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from imutils.video import FPS\n",
    "from imutils.video import FileVideoStream\n",
    "import imutils\n",
    "\n",
    "video = 'test.mp4'\n",
    "args = {'input': video, 'edge_detector': './holistically-nested-edge-detection/hed_model'}\n",
    "\n",
    "roots = './holistically-nested-edge-detection/hed_model'\n",
    "protoPath = os.path.join(args['edge_detector'], 'deploy.prototxt')\n",
    "modelPath = os.path.join(args['edge_detector'], 'hed_pretrained_bsds.caffemodel')\n",
    "\n",
    "# open a pointer to the video stream and start the FPS timer\n",
    "# vs = cv2.VideoCapture(args['input'])\n",
    "fvs = FileVideoStream(args['input']).start()\n",
    "fps = FPS().start()\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# register our new layer with the model\n",
    "cv2.dnn_registerLayer('Crop', CropLayer)\n",
    "\n",
    "# loop over frames from the video stream\n",
    "# while True:\n",
    "while fvs.more(): \n",
    "    \n",
    "    # grab the next frame and handle if we are reading from either videocapture or video stream\n",
    "    # (grabbed, frame) = vs.read()\n",
    "    # frame = frame[1]\n",
    "    frame = fvs.read()\n",
    "\n",
    "    # # if we are viewing a video and we did not grab a frame then we have reached the end of the video\n",
    "    # if not grabbed:\n",
    "    #     break\n",
    "\n",
    "    # load the input image and grab its dimensions\n",
    "    frame = imutils.resize(frame, width = 500)\n",
    "    (H, W) = frame.shape[:2]\n",
    "\n",
    "    # convert the image to grayscale, blur it, and perform Canny edge detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "    canny = cv2.Canny(blurred, 30, 150)\n",
    "\n",
    "    # construct a blob out of the input image for the Holistically-Nested Edge Detector\n",
    "    blob = cv2.dnn.blobFromImage(frame, scalefactor = 1.0, size = (W, H), mean = (104.00698793, 116.66876762, 122.67891434), swapRB = False, crop = False)\n",
    "\n",
    "    # set the blob as the input to the network and perform a forward pass to compute the edges\n",
    "    net.setInput(blob)\n",
    "    hed = net.forward()\n",
    "    hed = cv2.resize(hed[0, 0], (W, H))\n",
    "    hed = (255 * hed).astype('uint8')\n",
    "\n",
    "    # show the output edge detection results for Canny and Holistically-Nested Edge Detection\n",
    "    cv2.imshow('Frame', frame)\n",
    "    cv2.imshow('Canny', canny)\n",
    "    cv2.imshow('HED', hed)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    fps.update()\n",
    "    \n",
    "    # if the 'q' key was pressed, break from the loop\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "fps.stop()\n",
    "# vs.release()\n",
    "cv2.destroyAllWindows()\n",
    "fvs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto Canny edge detection (no parameter)\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2015/04/06/zero-parameter-automatic-canny-edge-detection-with-python-and-opencv/)\n",
    "\n",
    "The main method is to set a sigma, which set as .33 according to author's experience, and set the lower and the upper by +/ - the sigma times the median of the image data (np.median(gray_image)) <- this is already set as the gray one. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3/17 Build Mobile Document Scanner\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2014/09/01/build-kick-ass-mobile-document-scanner-just-5-minutes/)\n",
    "\n",
    "code is here -> open_cv_3_17.py\n",
    "\n",
    "- Detect edges\n",
    "\n",
    "- use the edges to find contour representing the piece of paper being scanned\n",
    "\n",
    "- apply perspective transform to obtain the top-down view \n",
    "\n",
    "## Some Situation not Suitable\n",
    "\n",
    "- make sure the shape is CLOSED RECTANGLE\n",
    "\n",
    "- make sure the background is distinct with the foreground rectangle image. \n",
    "\n",
    "## Some adjust suggestion: \n",
    "\n",
    "- [use dilate help to make close rectangle](https://stackoverflow.com/questions/43009923/how-to-complete-close-a-contour-in-python-opencv)\n",
    "\n",
    "- keep in mind, each step could adjust the threshold. So it's better to draw output in each key image. \n",
    "\n",
    "- also could add sharpened method before the final output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyimagesearch.transform import four_point_transform\n",
    "from skimage.filters import threshold_local\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imutils\n",
    "import sys\n",
    "\n",
    "args = {'image': 'check_scan_img.JPG'}\n",
    "\n",
    "# load the image and compute the ratio of the old height to the new height, clone it, and resize it\n",
    "image = cv2.imread(args['image'])\n",
    "cv2.startWindowThread()\n",
    "ratio = image.shape[0] / 500.0\n",
    "orig = image.copy()\n",
    "image = imutils.resize(image, height = 500)\n",
    "\n",
    "# convert the image to grayscale, blur it and find edges in the image\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "gray = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "edged = cv2.Canny(gray, 75, 200)\n",
    "\n",
    "# this is to use the dilated to make a closed rectangle\n",
    "# to connect the contour\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "# for kindle pic 5, 5\n",
    "dilated = cv2.dilate(edged, kernel)\n",
    "\n",
    "\n",
    "# show the original image and the edge detected image\n",
    "print ('STEP 1: Edge Detection')\n",
    "cv2.imshow('Image', image)\n",
    "cv2.imshow('Edged', edged)\n",
    "cv2.imshow('Dilated', dilated)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the controus in the edged image, keeping only the largest ones, and initialize the screen contour\n",
    "\n",
    "cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE) # use edged or dilated according to the actual situation\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]\n",
    "\n",
    "# loop over the contours\n",
    "for ind, c in enumerate(cnts): \n",
    "    # approximate the contour\n",
    "    peri = cv2.arcLength(c, True) # <- 计算封闭周长\n",
    "    approx = cv2.approxPolyDP(c, .02 * peri, True) # <- 计算多边形有多少条边\n",
    "\n",
    "    # if our approximated contour has four points, then we can assume that we have found our screen\n",
    "    if  len(approx) == 4: \n",
    "        screenCnt = approx\n",
    "        break\n",
    "    \n",
    "    # if all the cnt is checked but no len(approx) == 4, print the message and quit the python script\n",
    "    if ind == len(cnts) - 1: \n",
    "        print ('no rectangle found, please check the source image. ')\n",
    "        sys.exit(0)\n",
    "    \n",
    "# show the contour (outline) of the piece of paper\n",
    "print ('STEP 2: Find contours of paper')\n",
    "cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)\n",
    "cv2.imshow('Outline', image)\n",
    "cv2.startWindowThread()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply perspective transform to obtain the top-down view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the four point transform to obtain a top-down view of the original image\n",
    "warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
    "warped = add_sharpen_kernel(warped, False) # personal defined\n",
    "\n",
    "# convert the warped image to grayscale, then threshold it to give it that 'black and white' paper effect\n",
    "warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "T = threshold_local(warped, 11, offset = 20, method = 'gaussian') # (warped, 11, offset = 10, method = 'gaussian')\n",
    "warped = (warped > T).astype('uint8') * 255 \n",
    "# show the original and scanned images\n",
    "print ('STEP 3: Apply perspective transform')\n",
    "cv2.imshow('Original', imutils.resize(orig, height = 650))\n",
    "cv2.imshow('Scanned', imutils.resize(warped, height = 650))\n",
    "\n",
    "cv2.startWindowThread()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply a Perspective Transform & Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the four point transform to obtain a top-down view of the original image\n",
    "warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)\n",
    "\n",
    "# convert the warped image to grayscale, then threshold it to give it that 'black and white' paper effect\n",
    "warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)\n",
    "T = threshold_local(warped, 11, offset = 10, method = 'gaussian')\n",
    "warped = (warped > T).astype('uint8') * 255\n",
    "\n",
    "# show the original and scanned images\n",
    "print ('STEP 3: Apply perspective transform')\n",
    "cv2.imshow('Original', imutils.resize(orig, height = 650))\n",
    "cv2.startWindowThread()\n",
    "cv2.imshow('Scanned', imutils.resize(warped, height = 650))\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4/17 Bubble sheet multiple choice scanner and test grader using OMR (Optical Mark Recognition), etc\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2016/10/03/bubble-sheet-multiple-choice-scanner-and-test-grader-using-omr-python-and-opencv/)\n",
    "\n",
    "code is here -> ./open_cv_4_17.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5/17 Ball Tracking\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2015/09/14/ball-tracking-with-opencv/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6/17 Measure Size of Objects\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2016/03/28/measuring-size-of-objects-in-an-image-with-opencv/?__s=yo68x506yucrfbb1gtj5)\n",
    "\n",
    "- get the pixels per metric ratio\n",
    "\n",
    "- fix the reference object in most left place etc\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8/19 Facial Detect\n",
    "\n",
    "Note: 7/19 no special content\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/?__s=yo68x506yucrfbb1gtj5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9/17 Eye Blink\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2017/04/24/eye-blink-detection-opencv-python-dlib/?__s=yo68x506yucrfbb1gtj5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10/17 Drowsiness Detection\n",
    "\n",
    "Notice: 11/17 is a introduction about computer vision, and ads\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2017/05/08/drowsiness-detection-opencv/)\n",
    "\n",
    "[raspberry_link](https://www.pyimagesearch.com/2017/10/23/raspberry-pi-facial-landmarks-drowsiness-detection-with-opencv-and-dlib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12/17 Simple Neural Networks\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2016/09/26/a-simple-neural-network-with-python-and-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14/17 create custom deep learning datasets \n",
    "\n",
    "[link_bing_image_api_built_dataset](https://www.pyimagesearch.com/2018/04/09/how-to-quickly-build-a-deep-learning-image-dataset/)\n",
    "\n",
    "[googl_image_built_db](https://www.pyimagesearch.com/2017/12/04/how-to-create-a-deep-learning-dataset-using-google-images/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15/17 build a CNN on the custom dataset \n",
    "\n",
    "[link](https://www.pyimagesearch.com/2018/04/16/keras-and-convolutional-neural-networks-cnns/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16/17 real-time object detection w/ deep learning \n",
    "\n",
    "[link1](https://www.pyimagesearch.com/2017/09/18/real-time-object-detection-with-deep-learning-and-opencv/)\n",
    "\n",
    "[link2](https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Inpainting (图像修复)\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2020/05/18/image-inpainting-with-opencv-and-python/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read barcode\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2018/05/21/an-opencv-barcode-and-qr-code-scanner-with-zbar/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster video file FPS w cv2.VideoCapture \n",
    "\n",
    "[link](https://www.pyimagesearch.com/2017/02/06/faster-video-file-fps-with-cv2-videocapture-and-opencv/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileVideoStream: \n",
    "    def __init__(self, path, queueSize = 128): \n",
    "        # initalize the file video stream along with the boolean used to indicate if the thread should be stopped or not\n",
    "        self.stream = cv2.VideoCapture(path)\n",
    "        self.stopped = False\n",
    "\n",
    "        # initialize the queue used to store frames read from the video file\n",
    "        self.Q = Queue(maxsize = queueSize)\n",
    "    \n",
    "    def start(self):\n",
    "        # start a thread to read frames from the video stream\n",
    "        t = Thread(target = self.update, args = ())\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        return self\n",
    "\n",
    "    def update(self): \n",
    "\n",
    "        # keep looping infinitely\n",
    "        while True: \n",
    "            # if the thread indicator variable is set, stop the thread\n",
    "            if self.stopped: \n",
    "                return \n",
    "\n",
    "            # otherwise, ensure the que has room in it\n",
    "            if not self.Q.full(): \n",
    "\n",
    "                # read the next frame from the file\n",
    "                (grabbed, frame) = self.stream.read()\n",
    "\n",
    "                # if the grabbed boolean is False, then we have reached the end of the video file\n",
    "                if not grabbed:\n",
    "                    self.stop()\n",
    "                    return \n",
    "                \n",
    "                # add the frame to the queue\n",
    "                self.Q.put(frame)\n",
    "    def read(self): \n",
    "        # return next frame in the queue\n",
    "        return self.Q.get()\n",
    "    \n",
    "    def more(self): \n",
    "        # return True if there are still frames in the queue\n",
    "        return self.Q.qsize() > 0\n",
    "    \n",
    "    def stop(self): \n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True\n",
    "def auto_canny(image, sigma = .33): \n",
    "    # compute the median of the single channel pixel intensities\n",
    "    v = np.median(image)\n",
    "\n",
    "    # apply automatic Canny edge detection using the computed media\n",
    "    lower = int(max(0, (1.0 - sigma) * v))\n",
    "    upper = int(min(255, (1.0 + sigma) * v))\n",
    "\n",
    "    edged = cv2.Canny(image, lower, upper)\n",
    "\n",
    "    return edged\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36564bitff28b0bc308b43728a982148efad7fb6",
   "display_name": "Python 3.6.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}