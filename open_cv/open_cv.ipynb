{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Detection w/ web stream| photo| video\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2018/02/26/face-detection-with-opencv-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After opencv 3.3, Ryabnikov includes accurate, deep learning based face detector [face_detector](https://github.com/opencv/opencv/tree/master/samples/dnn/face_detector)\n",
    "\n",
    "need two file: \n",
    "\n",
    "- .prototxt: defines the model architecture\n",
    "\n",
    "- .caffemodel: contains the weights for the actual layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Detection for Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "32"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "args = {'prototxt': './simple-object-tracking/deploy.prototxt', \n",
    "    'model': './simple-object-tracking/res10_300x300_ssd_iter_140000.caffemodel', \n",
    "    'image':'./test.jpeg', \n",
    "    'confidence': .5}\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(args['prototxt'], args['model'])\n",
    "\n",
    "# load the input image and construct an input blob for the image by resizing to a fixed 300 * 300 pixels and then normalizing it\n",
    "image = cv2.imread(args['image'])\n",
    "(h, w) = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "# pass the blob through the network and obtain the detections and predictions\n",
    "net.setInput(blob)\n",
    "detections = net.forward()\n",
    "\n",
    "# loop over the detections \n",
    "for i in range(0, detections.shape[2]): \n",
    "\n",
    "    # extract the confidence associated with the prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    # filter out weak detections by ensuring the 'confidence' is greater than the minimum confidence\n",
    "    if confidence > args['confidence']: \n",
    "\n",
    "        # compute the (x, y) - coordinates of the bounding box for the object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype('int')\n",
    "\n",
    "        # draw the bounding box of the face along with the associated probability\n",
    "        text = '{:.2f}%'.format(confidence * 100)    \n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "        cv2.putText(image, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, .45, (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow('Output', image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face detection for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "args = {'prototxt': './simple-object-tracking/deploy.prototxt', \n",
    "    'model': './simple-object-tracking/res10_300x300_ssd_iter_140000.caffemodel', \n",
    "    'video':'./test.mp4', \n",
    "    'confidence': .5}\n",
    "\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(args['prototxt'], args['model'])\n",
    "\n",
    "# open a pointer to the video stream and start the FPS timer\n",
    "stream = cv2.VideoCapture(args['video'])\n",
    "fps = FPS().start()\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "\n",
    "    # grab the frame from the threaded video file stream\n",
    "    (grabbed, frame) = stream.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end of the stream\n",
    "    if not grabbed: \n",
    "        break\n",
    "\n",
    "    # resize the frame and covert it to grayscale (while still retaining 3 channels)\n",
    "    frame = imutils.resize(frame, width = 400)\n",
    "\n",
    "    # grab the frame dimensions and covert it to a blob\n",
    "    (h, w) = frame.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "\n",
    "    # pass the blob throught the network and obtain the detections and preditions\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "\n",
    "    #loop over the detections\n",
    "    for i in range(0, detections.shape[2]): \n",
    "\n",
    "        # extract the confidence associated with the prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        # filter out weak detections by ensuring the 'confidence' is greater than the minimum confidence\n",
    "        if confidence < args['confidence']: \n",
    "            continue\n",
    "        \n",
    "        # compute the (x, y)- coordinates of the bounding obx for the object\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype('int')\n",
    "\n",
    "        # draw the bounding box of the face along with the associated probability\n",
    "        text = '{:.2f}%'.format(confidence * 100)    \n",
    "        y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "        cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "        cv2.putText(frame, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, .45, (0, 0, 255), 2)\n",
    "\n",
    "    # show the frame and update the FPS counter\n",
    "    cv2.imshow('Frame', frame)\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    fps.update()\n",
    "\n",
    "    # if the 'q' key was pressed, break from the loop\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "# stop the timer and display FPS information\n",
    "fps.stop()\n",
    "stream.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with Images\n",
    "\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2018/07/19/opencv-tutorial-a-guide-to-learn-opencv/)\n",
    "\n",
    "- deal cv2 and imutils with basic image skills\n",
    "\n",
    "- deal cv2 with the erode and mask etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection\n",
    "\n",
    "[link](https://www.pyimagesearch.com/2019/03/04/holistically-nested-edge-detection-with-opencv-and-deep-learning/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holistically-Nested Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropLayer(object): \n",
    "    def __init__(self, params, blobs): \n",
    "        # initialize our starting and ending (x, y) - coordinates of the crop\n",
    "        self.startX = 0\n",
    "        self.startY = 0\n",
    "        self.endX = 0\n",
    "        self.endY = 0\n",
    "    \n",
    "    def getMemoryShapes(self, inputs): \n",
    "        # the crop layer will receive two inputs -- we need to crop the first input blob to match the shape of the second one, keeping the batch size and number of channels\n",
    "        (inputShape, targetShape) = (inputs[0], inputs[1])\n",
    "        (batchSize, numChannels) = (inputShape[0], inputShape[1])\n",
    "        (H, W) = (targetShape[2], targetShape[3])\n",
    "\n",
    "        # compute the starting and ending crop coordinates\n",
    "        self.startX = int((inputShape[3] - targetShape[3]) / 2)\n",
    "        self.startY = int((inputShape[2] - targetShape[2]) / 2)\n",
    "        self.endX = self.startX + W\n",
    "        self.endY = self.startY + H\n",
    "\n",
    "        # return the shape of the volume (we will preform the actual crop during the forward pass)\n",
    "        return [[batchSize, numChannels, H, W]]\n",
    "    \n",
    "    def forward(self, inputs): \n",
    "        # use the derived (x, y)-coordinates to perform the crop\n",
    "        return [inputs[0][:, :, self.startY:self.endY, self.startY: self.endX]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "args = {'image': 'test_1.jpeg', 'edge_detector': './holistically-nested-edge-detection/hed_model'}\n",
    "\n",
    "roots = './holistically-nested-edge-detection/hed_model'\n",
    "protoPath = os.path.join(args['edge_detector'], 'deploy.prototxt')\n",
    "modelPath = os.path.join(args['edge_detector'], 'hed_pretrained_bsds.caffemodel')\n",
    "\n",
    "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "# register our new layer with the model\n",
    "cv2.dnn_registerLayer('Crop', CropLayer)\n",
    "\n",
    "# load the input image and grab its dimensions\n",
    "image = cv2.imread(args['image'])\n",
    "(H, W) = image.shape[:2]\n",
    "\n",
    "# convert the image to grayscale, blur it, and perform Canny edge detection\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "canny = cv2.Canny(blurred, 30, 150)\n",
    "\n",
    "# construct a blob out of the input image for the Holistically-Nested Edge Detector\n",
    "blob = cv2.dnn.blobFromImage(image, scalefactor = 1.0, size = (W, H), mean = (104.00698793, 116.66876762, 122.67891434), swapRB = False, crop = False)\n",
    "\n",
    "# set the blob as the input to the network and perform a forward pass to compute the edges\n",
    "net.setInput(blob)\n",
    "hed = net.forward()\n",
    "hed = cv2.resize(hed[0, 0], [W, H])\n",
    "hed = (255 * hed).astype('uint8')\n",
    "\n",
    "# show the output edge detection results for Canny and Holistically-Nested Edge Detection\n",
    "cv2.imshow('Input', image)\n",
    "cv2.imshow('Canny', canny)\n",
    "cv2.imshow('HED', hed)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36564bitff28b0bc308b43728a982148efad7fb6",
   "display_name": "Python 3.6.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}